{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPQlTrXlINM202PZm3Y5+Uw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msharovar/ECGR6119/blob/main/midterm_gan_msharova.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyqMbWdeWpsd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Convolution2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from random import seed\n",
        "from random import random\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm\n",
        "\n",
        "def residual_block_generator():\n",
        "  model=tf.keras.Sequential([\n",
        "  tf.keras.layers.Conv2D(64,3,strides=(1,1),padding='same'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.LeakyReLU(),\n",
        "  tf.keras.layers.Conv2D(64,3,strides=(1,1),padding='same'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.LeakyReLU(),\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def generator_model():\n",
        "  # k9n64s1\n",
        "  input = tf.keras.layers.Input(shape=(None,None,3))\n",
        "  input_conv_layer = tf.keras.layers.Conv2D(64,9,padding='same')(input)\n",
        "  input_conv_layer = tf.keras.layers.LeakyReLU()(input_conv_layer)\n",
        "  output_layer_1 = input_conv_layer\n",
        "\n",
        "  # Five Residual Blocks k3n64s1\n",
        "  for x in range(5):\n",
        "    residual_output = residual_block_generator()(output_layer_1)\n",
        "    # Elementwise Sum\n",
        "    output_layer_1 = tf.keras.layers.Add()([output_layer_1,residual_output])\n",
        "\n",
        "  # k3n64s1\n",
        "  output_layer_1 = tf.keras.layers.Conv2D(64,9,padding='same')(output_layer_1)\n",
        "  output_layer_1 = tf.keras.layers.BatchNormalization()(output_layer_1)\n",
        "  # Elementwise Sum\n",
        "  output_layer_1 = tf.keras.layers.Add()([output_layer_1,input_conv_layer])\n",
        "\n",
        "  # Upsample Block 1 k3n256s1\n",
        "  upsample_layer_1 = tf.keras.layers.Conv2D(256,3, strides=(1,1),padding='same')(output_layer_1)\n",
        "  upsample_layer_1 = tf.nn.depth_to_space(upsample_layer_1, 2)\n",
        "  upsample_layer_1 = tf.keras.layers.LeakyReLU()(upsample_layer_1)\n",
        "\n",
        "  # Upsample Block 2 k3n256s1\n",
        "  upsample_layer_2 = tf.keras.layers.Conv2D(256,3, strides=(1,1),padding='same')(upsample_layer_1)\n",
        "  upsample_layer_2 = tf.nn.depth_to_space(upsample_layer_2, 2)\n",
        "  upsample_layer_2 = tf.keras.layers.LeakyReLU()(upsample_layer_2)\n",
        "\n",
        "  # Output layer k9n3s1\n",
        "  output_layer_2 = tf.keras.layers.Conv2D(3,9,activation='tanh',padding='same')(upsample_layer_2)\n",
        "  generator = tf.keras.models.Model(input, output_layer_2)\n",
        "\n",
        "  #generator.summary()\n",
        "  return generator\n",
        "\n",
        "def discriminator_model():\n",
        "  # k3n64s1\n",
        "  input = tf.keras.layers.Input(shape=(128,128,3))\n",
        "  input_conv_layer = tf.keras.layers.Conv2D(64,3,padding='same')(input)\n",
        "  input_conv_layer = tf.keras.layers.LeakyReLU()(input_conv_layer)\n",
        "\n",
        "  # Residual Layer 1 k3n64s2\n",
        "  residual_layer_1 = tf.keras.layers.Conv2D(64,3,strides=(2,2),padding='same')(input_conv_layer)\n",
        "  residual_layer_1 = tf.keras.layers.BatchNormalization()(residual_layer_1)\n",
        "  residual_layer_1 = tf.keras.layers.LeakyReLU()(residual_layer_1)\n",
        "\n",
        "  # Residual Layer 2 k3n128s1\n",
        "  residual_layer_2 = tf.keras.layers.Conv2D(128,3,strides=(1,1),padding='same')(residual_layer_1)\n",
        "  residual_layer_2 = tf.keras.layers.BatchNormalization()(residual_layer_2)\n",
        "  residual_layer_2 = tf.keras.layers.LeakyReLU()(residual_layer_2)\n",
        "\n",
        "  # Residual Layer 3 k3n128s2\n",
        "  residual_layer_3 = tf.keras.layers.Conv2D(128,3,strides=(2,2),padding='same')(residual_layer_2)\n",
        "  residual_layer_3 = tf.keras.layers.BatchNormalization()(residual_layer_3)\n",
        "  residual_layer_3 = tf.keras.layers.LeakyReLU()(residual_layer_3)\n",
        "\n",
        "  # Residual Layer 4 k3n256s1\n",
        "  residual_layer_4 = tf.keras.layers.Conv2D(256,3,strides=(1,1),padding='same')(residual_layer_3)\n",
        "  residual_layer_4 = tf.keras.layers.BatchNormalization()(residual_layer_4)\n",
        "  residual_layer_4 = tf.keras.layers.LeakyReLU()(residual_layer_4)\n",
        "\n",
        "  # Residual Layer 5 k3n256s2\n",
        "  residual_layer_5 = tf.keras.layers.Conv2D(256,3,strides=(2,2),padding='same')(residual_layer_4)\n",
        "  residual_layer_5 = tf.keras.layers.BatchNormalization()(residual_layer_5)\n",
        "  residual_layer_5 = tf.keras.layers.LeakyReLU()(residual_layer_5)\n",
        "\n",
        "  # Residual Layer 6 k3n512s1\n",
        "  residual_layer_6 = tf.keras.layers.Conv2D(512,3,strides=(1,1),padding='same')(residual_layer_5)\n",
        "  residual_layer_6 = tf.keras.layers.BatchNormalization()(residual_layer_6)\n",
        "  residual_layer_6 = tf.keras.layers.LeakyReLU()(residual_layer_6)\n",
        "\n",
        "  # Residual Layer 7 k3n512s2\n",
        "  residual_layer_7 = tf.keras.layers.Conv2D(512,3,strides=(2,2),padding='same')(residual_layer_6)\n",
        "  residual_layer_7 = tf.keras.layers.BatchNormalization()(residual_layer_7)\n",
        "  residual_layer_7 = tf.keras.layers.LeakyReLU()(residual_layer_7)\n",
        "\n",
        "  output_layer = tf.keras.layers.Flatten()(residual_layer_7)\n",
        "  output_layer = tf.keras.layers.Dense(1024)(output_layer)\n",
        "  output_layer = tf.keras.layers.LeakyReLU()(output_layer)\n",
        "  output_layer = tf.keras.layers.Dense(1,activation='sigmoid')(output_layer)\n",
        "  discriminator = tf.keras.models.Model(input,output_layer)\n",
        "\n",
        "  #discriminator.summary()\n",
        "  return discriminator\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def build_data(data_image, data_label):\n",
        "  print(data_image, data_label)\n",
        "  image = tf.image.resize(data_image, [128,128])\n",
        "  image = tf.divide(image, 255)\n",
        "  print(image)\n",
        "  #cropped=tf.dtypes.cast(tf.image.random_crop(data['image'] / 255,(128,128,3)),tf.float32)\n",
        "  lr=tf.image.resize(image,(32,32))\n",
        "  return (lr,image * 2 - 1)\n",
        "\n",
        "def train_step(data,adv_ratio=0.001):\n",
        "  gen_loss,disc_loss=0,0\n",
        "  low_resolution,high_resolution=data\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    super_resolution = generator(low_resolution, training=True)\n",
        "    gen_loss = tf.reduce_mean( (high_resolution - super_resolution) ** 2 )\n",
        "\n",
        "    real_output = discriminator(high_resolution, training=True)\n",
        "    fake_output = discriminator(super_resolution, training=True)\n",
        "    adv_loss_g = generator_loss(fake_output) * adv_ratio\n",
        "    gen_loss += adv_loss_g\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "# get dataset that contains images of cats and dogs\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    \"cats_vs_dogs\",\n",
        "    # Reserve 10% for validation (from 50% to 60%) and 10% for test (from 60% to 70%)\n",
        "    split=[\"train[:50%]\", \"train[50%:60%]\", \"train[60%:70%]\"],\n",
        "    as_supervised=True)\n",
        "\n",
        "ds_img_100_to_120 = train_data.skip(100).take(20)\n",
        "# display\n",
        "for i, (image, label) in enumerate(iterable = ds_img_100_to_120, start = 0):\n",
        "    ax = plt.subplot(5, 4, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(int(label))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "generator = generator_model()\n",
        "discriminator = discriminator_model()\n",
        "\n",
        "generator_optimizer=tf.keras.optimizers.Adam(0.001)\n",
        "discriminator_optimizer=tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "#latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "\n",
        "noise = tf.random.normal([1, 128, 128, 3])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "\n",
        "for epoch in range(150):\n",
        "  train_dataset_mapped = train_data.map(build_data,num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
        "  val_dataset_mapped = test_data.map(build_data,num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
        "\n",
        "  for image_batch in tqdm(train_dataset_mapped, position=0, leave=True):\n",
        "    train_step(image_batch)\n",
        "  # Save the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  predictions = generator(noise, training=False)\n",
        "  fig = plt.figure(figsize=(32, 32))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(32, 32, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0])\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "\n",
        "noise = tf.random.normal([1, 128, 128, 3])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "h9Qc-4h6W2aT"
      }
    }
  ]
}